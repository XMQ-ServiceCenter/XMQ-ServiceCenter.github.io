<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.2.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Serif+Simplified+Chinese:300,300italic,400,400italic,700,700italic%7CMicrosoft+YaHei:300,300italic,400,400italic,700,700italic%7CNoto+Serif+SC:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/all.min.css" integrity="sha256-XOqroi11tY4EFQMR9ZYwZWKj5ZXiftSx36RRuC3anlA=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"xmq-servicecenter.github.io","root":"/","images":"/images","scheme":"Pisces","darkmode":false,"version":"8.20.0","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"copycode":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="优化问题的大致概念，以及各类优化算法的大致思路。  优化问题的极小条件 梯度的直观图像 函数 \(f(x)\) 的梯度 \(\nabla f(x)\) 反映了这个函数上升最快的方向，大致的理解思路如下：考虑 \(f(x)\) 在 \(x_0\) 附近的泰勒展开 \[f(x_0+\alpha p)\approx f(x_0)+\alpha p^{\mathrm T}\nabla f(x_">
<meta property="og:type" content="article">
<meta property="og:title" content="数值优化1">
<meta property="og:url" content="https://xmq-servicecenter.github.io/2024/06/01/Optimization-1/index.html">
<meta property="og:site_name" content="XMQ-维修中心">
<meta property="og:description" content="优化问题的大致概念，以及各类优化算法的大致思路。  优化问题的极小条件 梯度的直观图像 函数 \(f(x)\) 的梯度 \(\nabla f(x)\) 反映了这个函数上升最快的方向，大致的理解思路如下：考虑 \(f(x)\) 在 \(x_0\) 附近的泰勒展开 \[f(x_0+\alpha p)\approx f(x_0)+\alpha p^{\mathrm T}\nabla f(x_">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://xmq-servicecenter.github.io/picture/Wolfe_condition.png">
<meta property="article:published_time" content="2024-05-31T16:00:00.000Z">
<meta property="article:modified_time" content="2024-07-25T01:57:46.167Z">
<meta property="article:author" content="XMQ">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://xmq-servicecenter.github.io/picture/Wolfe_condition.png">


<link rel="canonical" href="https://xmq-servicecenter.github.io/2024/06/01/Optimization-1/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://xmq-servicecenter.github.io/2024/06/01/Optimization-1/","path":"2024/06/01/Optimization-1/","title":"数值优化1"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>数值优化1 | XMQ-维修中心</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">XMQ-维修中心</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-考前预习"><a href="/%E8%80%83%E5%89%8D%E9%A2%84%E4%B9%A0/" rel="section"><i class="fa fa-th fa-fw"></i>考前预习</a></li>
  </ul>
</nav>




</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%BC%98%E5%8C%96%E9%97%AE%E9%A2%98%E7%9A%84%E6%9E%81%E5%B0%8F%E6%9D%A1%E4%BB%B6"><span class="nav-number">1.</span> <span class="nav-text">优化问题的极小条件</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E7%9A%84%E7%9B%B4%E8%A7%82%E5%9B%BE%E5%83%8F"><span class="nav-number">1.1.</span> <span class="nav-text">梯度的直观图像</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%97%A0%E7%BA%A6%E6%9D%9F%E4%BC%98%E5%8C%96%E9%97%AE%E9%A2%98%E7%9A%84%E5%B1%80%E9%83%A8%E6%9E%81%E5%B0%8F%E6%9D%A1%E4%BB%B6"><span class="nav-number">1.2.</span> <span class="nav-text">无约束优化问题的局部极小条件</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#kkt-%E6%9D%A1%E4%BB%B6"><span class="nav-number">1.3.</span> <span class="nav-text">KKT 条件</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BA%A6%E6%9D%9F%E4%BC%98%E5%8C%96%E9%97%AE%E9%A2%98%E7%9A%84%E4%B8%80%E9%98%B6%E5%BF%85%E8%A6%81%E6%9D%A1%E4%BB%B6"><span class="nav-number">1.4.</span> <span class="nav-text">约束优化问题的一阶必要条件</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#sfd"><span class="nav-number">1.4.1.</span> <span class="nav-text">SFD</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#lfd"><span class="nav-number">1.4.2.</span> <span class="nav-text">LFD</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#kkt-%E5%AE%9A%E7%90%86"><span class="nav-number">1.4.3.</span> <span class="nav-text">KKT 定理</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%B8%A4%E7%A7%8D%E8%BF%AD%E4%BB%A3%E6%B3%95%E7%9A%84%E6%80%9D%E8%B7%AF"><span class="nav-number">2.</span> <span class="nav-text">两种迭代法的思路</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%9D%9E%E7%B2%BE%E7%A1%AE%E7%BA%BF%E6%90%9C%E7%B4%A2%E7%9A%84%E5%88%A4%E5%AE%9A%E6%9D%A1%E4%BB%B6"><span class="nav-number">2.1.</span> <span class="nav-text">非精确线搜索的判定条件</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BF%A1%E8%B5%96%E5%9F%9F%E6%96%B9%E6%B3%95"><span class="nav-number">2.2.</span> <span class="nav-text">信赖域方法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%80%E8%88%AC%E6%80%9D%E8%B7%AF"><span class="nav-number">2.2.1.</span> <span class="nav-text">一般思路</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%A6%E4%B8%80%E7%A7%8D%E6%80%9D%E8%B7%AFlevenberg-marquardt-%E6%96%B9%E6%B3%95"><span class="nav-number">2.2.2.</span> <span class="nav-text">另一种思路（Levenberg-Marquardt
方法）</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%97%A0%E7%BA%A6%E6%9D%9F%E4%BC%98%E5%8C%96%E9%97%AE%E9%A2%98%E7%9A%84%E6%8B%9F%E7%89%9B%E9%A1%BF%E6%B3%95"><span class="nav-number">3.</span> <span class="nav-text">无约束优化问题的拟牛顿法</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%89%9B%E9%A1%BF%E6%B3%95%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%80%9D%E8%B7%AF"><span class="nav-number">3.1.</span> <span class="nav-text">牛顿法的基本思路</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%8B%9F%E7%89%9B%E9%A1%BF%E6%B3%95%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%80%9D%E8%B7%AF"><span class="nav-number">3.2.</span> <span class="nav-text">拟牛顿法的基本思路</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%89%E7%A7%8D%E5%B8%B8%E7%94%A8%E7%9A%84%E4%BF%AE%E6%AD%A3"><span class="nav-number">3.3.</span> <span class="nav-text">三种常用的修正</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#sr1-%E4%BF%AE%E6%AD%A3"><span class="nav-number">3.3.1.</span> <span class="nav-text">SR1 修正</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#bfgs-%E4%B8%8E-dfp-%E4%BF%AE%E6%AD%A3"><span class="nav-number">3.3.2.</span> <span class="nav-text">BFGS 与 DFP 修正</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%97%A0%E7%BA%A6%E6%9D%9F%E4%BC%98%E5%8C%96%E9%97%AE%E9%A2%98%E7%9A%84%E5%85%B1%E8%BD%AD%E6%A2%AF%E5%BA%A6%E6%B3%95"><span class="nav-number">4.</span> <span class="nav-text">无约束优化问题的共轭梯度法</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BA%8C%E6%AC%A1%E5%87%BD%E6%95%B0%E7%9A%84%E5%85%B1%E8%BD%AD%E6%A2%AF%E5%BA%A6%E6%B3%95"><span class="nav-number">4.1.</span> <span class="nav-text">二次函数的共轭梯度法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9F%BA%E6%9C%AC%E6%80%9D%E8%B7%AF"><span class="nav-number">4.1.1.</span> <span class="nav-text">基本思路</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#p_k-%E7%9A%84%E9%80%89%E5%8F%96"><span class="nav-number">4.1.2.</span> <span class="nav-text">\(p_k\) 的选取</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#alpha_k-%E7%9A%84%E9%80%89%E5%8F%96"><span class="nav-number">4.1.3.</span> <span class="nav-text">\(\alpha_k\)
的选取</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%9D%9E%E4%BA%8C%E6%AC%A1%E5%87%BD%E6%95%B0%E7%9A%84%E5%85%B1%E8%BD%AD%E6%A2%AF%E5%BA%A6%E6%B3%95"><span class="nav-number">4.2.</span> <span class="nav-text">非二次函数的共轭梯度法</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%85%B6%E4%BB%96%E9%97%AE%E9%A2%98%E4%B8%8E%E7%AE%97%E6%B3%95"><span class="nav-number">5.</span> <span class="nav-text">其他问题与算法</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E9%97%AE%E9%A2%98"><span class="nav-number">5.1.</span> <span class="nav-text">最小二乘问题</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%80%E8%88%AC%E7%9A%84%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E9%97%AE%E9%A2%98"><span class="nav-number">5.1.1.</span> <span class="nav-text">一般的最小二乘问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BA%BF%E6%80%A7%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E9%97%AE%E9%A2%98"><span class="nav-number">5.1.2.</span> <span class="nav-text">线性最小二乘问题</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BA%BF%E6%80%A7%E8%A7%84%E5%88%92%E9%97%AE%E9%A2%98%E7%9A%84%E5%8D%95%E7%BA%AF%E5%BD%A2%E6%B3%95"><span class="nav-number">5.2.</span> <span class="nav-text">线性规划问题的单纯形法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BA%BF%E6%80%A7%E8%A7%84%E5%88%92%E9%97%AE%E9%A2%98"><span class="nav-number">5.2.1.</span> <span class="nav-text">线性规划问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8D%95%E7%BA%AF%E5%BD%A2%E6%B3%95%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%80%9D%E8%B7%AF"><span class="nav-number">5.2.2.</span> <span class="nav-text">单纯形法的基本思路</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BF%A1%E8%B5%96%E5%9F%9F%E7%9A%84-dogleg-%E6%96%B9%E6%B3%95"><span class="nav-number">5.3.</span> <span class="nav-text">信赖域的 dogleg 方法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BA%A6%E6%9D%9F%E4%BC%98%E5%8C%96%E9%97%AE%E9%A2%98%E7%9A%84%E5%86%85%E7%82%B9%E6%96%B9%E6%B3%95"><span class="nav-number">5.4.</span> <span class="nav-text">约束优化问题的内点方法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BA%A6%E6%9D%9F%E4%BC%98%E5%8C%96%E9%97%AE%E9%A2%98%E7%9A%84-kkt-%E6%9D%A1%E4%BB%B6"><span class="nav-number">5.4.1.</span> <span class="nav-text">约束优化问题的 KKT 条件</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AF%B9%E5%81%B6%E7%BD%9A%E5%87%BD%E6%95%B0%E7%9A%84-kkt-%E6%9D%A1%E4%BB%B6"><span class="nav-number">5.4.2.</span> <span class="nav-text">对偶罚函数的 KKT 条件</span></a></li></ol></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">XMQ</p>
  <div class="site-description" itemprop="description">奇怪的小网站</div>
</div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://xmq-servicecenter.github.io/2024/06/01/Optimization-1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="XMQ">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="XMQ-维修中心">
      <meta itemprop="description" content="奇怪的小网站">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="数值优化1 | XMQ-维修中心">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          数值优化1
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2024-06-01 00:00:00" itemprop="dateCreated datePublished" datetime="2024-06-01T00:00:00+08:00">2024-06-01</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2024-07-25 09:57:46" itemprop="dateModified" datetime="2024-07-25T09:57:46+08:00">2024-07-25</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><p>优化问题的大致概念，以及各类优化算法的大致思路。</p>
<hr />
<h1 id="优化问题的极小条件">优化问题的极小条件</h1>
<h2 id="梯度的直观图像">梯度的直观图像</h2>
<p>函数 <span class="math inline">\(f(x)\)</span> 的梯度 <span
class="math inline">\(\nabla f(x)\)</span>
反映了这个函数上升最快的方向，大致的理解思路如下：考虑 <span
class="math inline">\(f(x)\)</span> 在 <span
class="math inline">\(x_0\)</span> 附近的泰勒展开 <span
class="math display">\[f(x_0+\alpha p)\approx f(x_0)+\alpha p^{\mathrm
T}\nabla f(x_0),\]</span> 其中 <span class="math inline">\(p\)</span>
为某个单位矢量，<span class="math inline">\(\alpha&gt;0\)</span> 为在
<span class="math inline">\(p\)</span> 方向的步长。</p>
<p>等号右边第二项为向量内积，所以，当 <span
class="math inline">\(p\)</span> 与 <span class="math inline">\(\nabla
f(x_0)\)</span> 指向相同时 <span class="math inline">\(f(x)\)</span>
上升最显著，<span class="math inline">\(p\)</span> 与 <span
class="math inline">\(\nabla f(x_0)\)</span> 指向相反时 <span
class="math inline">\(f(x)\)</span> 下降最显著。</p>
<h2 id="无约束优化问题的局部极小条件">无约束优化问题的局部极小条件</h2>
<p>一个函数 <span class="math inline">\(f(x)\)</span> 在 <span
class="math inline">\(x^*\)</span>
处取得局部极小值的充分条件与必要条件有：</p>
<blockquote class="colorquote theorem" ><ul>
<li><p>一阶必要条件：如果函数 <span class="math inline">\(f(x)\)</span>
的一阶导是连续的，且 <span class="math inline">\(f(x)\)</span> 在 <span
class="math inline">\(x^*\)</span> 处取得局部极小，那么 <span
class="math inline">\(f(x)\)</span> 在 <span
class="math inline">\(x^*\)</span> 处的一阶导为 0，即 <span
class="math inline">\(\nabla f(x)|_{x=x_0}=0\)</span>。</p></li>
<li><p>二阶必要条件：如果函数 <span class="math inline">\(f(x)\)</span>
的二阶导是连续的，且 <span class="math inline">\(f(x)\)</span> 在 <span
class="math inline">\(x^*\)</span> 处取得局部极小，那么 <span
class="math inline">\(f(x)\)</span> 在 <span
class="math inline">\(x^*\)</span> 处满足一阶导 <span
class="math inline">\(\nabla f(x)|_{x=x^*}=0\)</span> 且二阶导 <span
class="math inline">\(\nabla^2f(x)|_{x=x^*}\)</span> 半正定。</p></li>
<li><p>二阶充分条件：如果函数 <span class="math inline">\(f(x)\)</span>
的二阶导连续，并且在 <span class="math inline">\(x^*\)</span>
处满足一阶导 <span class="math inline">\(\nabla
f(x)|_{x=x^*}=0\)</span>、二阶导 <span
class="math inline">\(\nabla^2f(x)|_{x=x^*}\)</span> 正定，那么 <span
class="math inline">\(x^*\)</span> 是 <span
class="math inline">\(f(x)\)</span> 的一个局部极小值点。</p></li>
</ul>
</blockquote>
<p>这些条件的理解方式很简单： <span
class="math display">\[f(x^\ast+\delta)=f(x^\ast)+\delta^{\mathrm
T}[\nabla f(x^\ast)]+\dfrac12\delta^{\mathrm T}[\nabla^2
f(x^\ast)]\delta+\cdots.\]</span> 然后比较 <span
class="math inline">\(f(x^\ast+\delta)\)</span> 与 <span
class="math inline">\(f(x^\ast)\)</span> 的大小即可：</p>
<ul>
<li><p>一阶必要条件：如果 <span class="math inline">\([\nabla
f(x^\ast)]\)</span> 不为 <span
class="math inline">\(0\)</span>，就必有某个与 <span
class="math inline">\([\nabla f(x^\ast)]\)</span> 呈钝角的 <span
class="math inline">\(\delta\)</span> 使 <span
class="math inline">\(f(x^\ast)\)</span> 下降。</p></li>
<li><p>二阶必要条件：如果 <span class="math inline">\([\nabla
f(x^\ast)]=0\)</span> 但 <span class="math inline">\([\nabla^2
f(x^\ast)]\)</span> 有负的特征值，就可以将 <span
class="math inline">\(\delta\)</span> 取为负特征值的特征向量使 <span
class="math inline">\(f(x^\ast)\)</span> 下降。</p></li>
<li><p>二阶充分条件：如果 <span class="math inline">\([\nabla^2
f(x^\ast)]\)</span> 正定，那么所有非零的 <span
class="math inline">\(\delta\)</span> 都会使得 <span
class="math inline">\(f(x^\ast+\delta)&gt;f(x^\ast)\)</span>。</p></li>
</ul>
<h2 id="kkt-条件">KKT 条件</h2>
<p>考虑以下形式的优化问题：</p>
<p><span class="math display">\[\text{最小化}\;f(x),\qquad
x\;\text{满足}\;\begin{cases}
E_i(x)=0:\quad i\in\mathcal E,\\
I_j(x)\geqslant0:\quad j\in\mathcal I.
\end{cases}\]</span></p>
<p>为此需要引入<strong>拉格朗日函数</strong></p>
<blockquote class="colorquote definition" ><p><span class="math display">\[\mathcal L(x,\lambda):=
f(x)-\sum_{i\in\mathcal E}\lambda_iE_i(x)-\sum_{j\in\mathcal
I}\lambda_jI_j(x).\]</span></p>
</blockquote>
<p>满足以下条件的点 <span class="math inline">\(x\)</span> 称为
<strong>KKT 点</strong>：</p>
<blockquote class="colorquote definition" ><ul>
<li><p><span class="math inline">\(\nabla_x\mathcal
L(x,\lambda)=0.\)</span></p></li>
<li><p>等式约束：<span class="math inline">\(E_i(x)=0\)</span> 或 <span
class="math inline">\(\nabla_{\lambda_i}\mathcal
L(x,\lambda)=0\)</span>，其中 <span class="math inline">\(i\in\mathcal
E.\)</span></p></li>
<li><p>不等式约束：<span class="math inline">\(I_j(x)\geqslant0\)</span>
且 <span class="math inline">\(\lambda_j\geqslant0\)</span> 且 <span
class="math inline">\(\lambda_jI_j(x)=0\)</span>，其中 <span
class="math inline">\(j\in\mathcal I.\)</span></p></li>
</ul>
</blockquote>
<p>在很多情况下，约束优化问题的极值点就是 KKT 点。</p>
<p>KKT 条件中，不等式约束的三个条件可通过如下方式理解：</p>
<ul>
<li><p>如果 <span class="math inline">\(I_j(x)&gt;0\)</span>，那么 <span
class="math inline">\(I_j(x)\geqslant0\)</span>
的条件实际上不起作用，这等价于没有这个约束，也就是 <span
class="math inline">\(\lambda_j=0\)</span>，所以不等式约束的三个条件都能满足。</p></li>
<li><p>如果 <span class="math inline">\(I_j(x)=0\)</span>，那么 <span
class="math inline">\(I_j(x)\geqslant0\)</span>
的条件起了作用，不等式约束的第一个和最后一个条件都能满足。所以问题的关键在于
<span class="math inline">\(\lambda_j\geqslant0\)</span>
的理解方式。</p></li>
</ul>
<p>首先，我们已经知道，梯度 <span class="math inline">\(\nabla
f(x)\)</span> 代表了 <span class="math inline">\(f(x)\)</span>
的上升方向，而 <span class="math inline">\(-\nabla f(x)\)</span>
代表了下降的方向。然后：</p>
<ul>
<li><p>如果 <span class="math inline">\(f(x)\)</span> 的极小值在
<strong>可行区域</strong>（即满足了所有约束条件的区域）的边界上取得，那么
<span class="math inline">\(-\nabla f(x)\)</span>
一定指向可行区域以外，否则其极小值就在可行区域内部取到。</p></li>
<li><p>可行区域由 <span class="math inline">\(I_j(x)\geqslant0\)</span>
确定 (<span class="math inline">\(i\in\mathcal I\)</span>)，因此 <span
class="math inline">\(\nabla I_j(x)\)</span>
一定指向可行区域内部，因为它代表了 <span
class="math inline">\(I_j(x)\)</span> 上升的方向。</p></li>
<li><p>所以 <span class="math inline">\(-\nabla f(x)\)</span> 与 <span
class="math inline">\(\nabla I_j(x)\)</span>
的方向在某种程度上应该是相反的，这就意味着，如果 <span
class="math inline">\(\nabla_x\mathcal L(x,\lambda)=0\)</span> 就必然有
<span class="math inline">\(\lambda_i\geqslant0\)</span>。</p></li>
</ul>
<p>需要注意的是，约束优化问题的极值点不一定是 KKT
点。例如有这样一个例子：</p>
<blockquote class="colorquote example" ><p><span class="math display">\[\begin{align}\label{LFD and SFD, KKT}
\text{最小化}\;f(x,y)=x,\qquad x,y\;\text{满足}
\begin{cases}
x^3-y\geqslant0\\
y\geqslant0
\end{cases}.
\end{align}\]</span></p>
</blockquote>
<p>不难想象，全局极小值点为 <span
class="math inline">\((0,0)\)</span>。这个例子的拉格朗日函数为 <span
class="math inline">\(\mathcal
L(x,y)=x-\lambda_1y-\lambda_2(x^3-y)\)</span>，由 <span
class="math inline">\(\partial\mathcal L/\partial x=0\)</span>
的条件可得 <span
class="math inline">\(1-3\lambda_2x^2=0\)</span>，全局极小值点 <span
class="math inline">\((0,0)\)</span> 不可能满足 KKT 条件。</p>
<h2 id="约束优化问题的一阶必要条件">约束优化问题的一阶必要条件</h2>
<p>要得到约束优化问题的一阶必要条件，需要先引入几个概念和引理：</p>
<blockquote class="colorquote definition" ><ul>
<li><p><strong>可行方向（FD, feasible direction）</strong>：给定 <span
class="math inline">\(x\)</span> 和方向 <span
class="math inline">\(d\)</span>，如果存在一个 <span
class="math inline">\(\delta&gt;0\)</span> 使得 <span
class="math inline">\(x\)</span> 到 <span
class="math inline">\((x+td)\)</span>
的连线上的所有的点都满足约束条件，就说 <span
class="math inline">\(d\)</span> 是 <span
class="math inline">\(x\)</span> 处的可行方向。</p></li>
<li><p><strong>序列可行方向（SFD）</strong>：如果存在序列 <span
class="math inline">\(\{(d_k,\delta_k)\}\)</span> 使得 <span
class="math inline">\(\delta_k&gt;0\)</span>，<span
class="math inline">\(\delta_k\rightarrow0\)</span>，<span
class="math inline">\(d_k\rightarrow d\)</span> 且 <span
class="math inline">\(x_k=x+\delta_kd_k\)</span>
一直满足约束条件，那么就说 <span class="math inline">\(d\)</span> 是
<span class="math inline">\(x\)</span> 的一个序列可行方向。</p></li>
<li><p><strong>线性化可行方向（LFD）</strong>：如果 <span
class="math inline">\(x\)</span> 处的某个方向 <span
class="math inline">\(d\)</span> 满足以下条件就说 <span
class="math inline">\(d\)</span> 是 <span
class="math inline">\(x\)</span> 的一个线性化可行方向： <span
class="math display">\[\begin{align*}
d^{\mathrm T}\nabla E_i(x)=0\;(i\in\mathcal E),\quad d^{\mathrm T}\nabla
I_i(x)\geqslant0\;(i\in\mathcal I),
\end{align*}\]</span></p></li>
</ul>
</blockquote>
<p>可以证明，这几个方向之间的关系为： <blockquote class="colorquote theorem" ><p><span class="math display">\[\begin{align*}
\text{FD}(x)\subseteq\text{SFD}(x)\subseteq\text{LFD}(x).
\end{align*}\]</span></p>
</blockquote></p>
<h3 id="sfd">SFD</h3>
<p>形象地来说，一个点处的 SFD
会形成一个“切锥”，它与约束条件的切线有关。例如约束条件 <span
class="math inline">\(x^2+y^2=1\)</span> 在 <span
class="math inline">\((1,0)\)</span> 处的切锥就是直线 <span
class="math inline">\(x=1\)</span> 代表的方向，约束条件 <span
class="math inline">\(-x^2-y^2\geqslant-1\)</span> 在 <span
class="math inline">\((1,0)\)</span> 处的切锥就是半平面 <span
class="math inline">\(x&lt;1\)</span>。</p>
<p>极小值点的一阶必要条件与序列可行化方向 SFD 相关：假设 <span
class="math inline">\(x^\ast\)</span> 是极小值点，然后考虑 <span
class="math display">\[\begin{align*}
\dfrac{f(x^\ast+\delta_k d_k)-f(x^\ast)}{\delta_k}=d_k^{\mathrm
T}[\nabla f(x^\ast)]+O(\delta_k),
\end{align*}\]</span> 两边取 <span
class="math inline">\(k\rightarrow\infty\)</span>
的极限就可得到约束优化问题的一阶必要条件：</p>
<blockquote class="colorquote theorem" ><p><span class="math display">\[\begin{align}\label{SFD-condition}
x^\ast\;\;\text{是}\;\;f(x) \text{ 的局部极小值点
}\quad\Rightarrow\quad\text{对任何 SFD 都有 }\;\;d^{\mathrm T}[\nabla
f(x^\ast)]\geqslant0.
\end{align}\]</span></p>
</blockquote>
<h3 id="lfd">LFD</h3>
<p>相较于 SFD，这里的 LFD
因为具有了明确的表达式而更好分析一些。但作为代价，LFD 相比 SFD
包含了一些额外的方向，这些方向可能指向可行区域以外：</p>
<blockquote class="colorquote example" ><ul>
<li>在 (<span class="math inline">\(\ref{LFD and SFD, KKT}\)</span>)
这个例子中，极小值点 <span class="math inline">\((0,0)\)</span> 处的 SFD
就是 <span class="math inline">\(\{(a,0):\;a&gt;0\}\)</span>，而 LFD 为
<span class="math inline">\(\{(a,0):\;a\in\mathbb R\}\)</span>，<span
class="math inline">\(\mathrm{LFD}\backslash\mathrm{SFD}\)</span>
中的方向就指向可行区域以外。</li>
<li>而在约束条件为 <span
class="math inline">\(-x^2-y^2\geqslant-1\)</span> 的例子中，<span
class="math inline">\(\mathrm{SFD}=\mathrm{LFD}\)</span>。</li>
</ul>
</blockquote>
<p>由于 LFD 可能包含指向可行区域以外的方向，所以存在这样的情形：极小值点
<span class="math inline">\(x^\ast\)</span> 点处的 SFD 中没有方向能使
<span class="math inline">\(f(x)\)</span> 下降，但 <span
class="math inline">\(x^\ast\)</span> 点处 LFD 中却有能让 <span
class="math inline">\(f(x)\)</span> 下降的方向，(<span
class="math inline">\(\ref{LFD and SFD, KKT}\)</span>)
就是这样的一个例子。</p>
<h3 id="kkt-定理">KKT 定理</h3>
<p>LFD 与 Farkas 引理相关。我们考虑 <span
class="math inline">\(x\)</span> 点处所有的具有如下形式的方向： <span
class="math display">\[\begin{align*}
S:= \left\lbrace u:\quad u=\sum_{i\in\mathcal E}\lambda_i\nabla
E_i(x)+\sum_{j\in\mathcal I}\lambda_j\nabla I_j(x)\right\rbrace.
\end{align*}\]</span> 不难发现，当 <span
class="math inline">\(x^\ast\)</span>
是我们要考虑的问题的极值点时，<span class="math inline">\(\nabla
f(x^\ast)\in S\)</span>。</p>
<p><span class="math inline">\(S\)</span>
中的方向可以分为两类，其中一类记为 <span
class="math inline">\(S_+\)</span>： <span
class="math display">\[\begin{align*}
S_+=\left\lbrace u=\sum_{i\in\mathcal E}\lambda_i\nabla
E_i(x)+\sum_{j\in\mathcal I}\lambda_j\nabla I_j(x):\quad
\lambda_j\geqslant0\;(\forall j \in \mathcal I)\right\rbrace ,
\end{align*}\]</span> 剩下的归为另一类。不难发现，<span
class="math inline">\(S_+\)</span> 是一个凸集（锥）。</p>
<p>对于 LFD 中的任意一个 <span
class="math inline">\(d\)</span>，不难想象： <span
class="math display">\[\begin{align*}
d^{\mathrm T}u=\sum_{j\in\mathcal I}\lambda_jd^{\mathrm T}\nabla
I_j(x)\geqslant0,\qquad\forall u\in S_+.
\end{align*}\]</span> 这意味着 <span class="math inline">\(S_+\)</span>
分布在以 <span class="math inline">\(d\)</span>
为法向量的超平面的一侧，不会跨过这一超平面，这就是 LFD
中的方向的几何含义。</p>
<p>现在，假设 <span class="math inline">\(\nabla f(x)\in
S\)</span>。</p>
<ul>
<li>如果存在一个 <span class="math inline">\(d\)</span> 让 <span
class="math inline">\(f(x)\)</span> 下降，即 <span
class="math inline">\(d^{\mathrm T}\nabla f&lt;0\)</span>，这就意味着
<span class="math inline">\(\nabla f\)</span> 与 <span
class="math inline">\(S_+\)</span> 分布在以 <span
class="math inline">\(d\)</span> 为法向量的超平面的两侧，也就是 <span
class="math inline">\(\nabla f\notin S_+\)</span>。</li>
<li>反之，如果 <span class="math inline">\(\nabla f\in S\)</span> 但
<span class="math inline">\(\nabla f\notin
S_+\)</span>，那么依据凸集分离定理总能找到一个平面将 <span
class="math inline">\(\nabla f\)</span> 与 <span
class="math inline">\(S_+\)</span> 分隔开，这个平面的指向 <span
class="math inline">\(S_+\)</span> 的法向量 <span
class="math inline">\(d\)</span> 满足 <span
class="math inline">\(d^{\mathrm T}\nabla f&lt;0\)</span>。</li>
</ul>
<p>这样就证明了：如果 <span class="math inline">\(\nabla f(x)\in
S\)</span>，那么 LFD 中存在一个方向 <span
class="math inline">\(d\)</span> 使 <span
class="math inline">\(d^{\mathrm T}\nabla f&lt;0\)</span> 当且仅当 <span
class="math inline">\(\nabla f\notin S_+\)</span>。因此，如果 <span
class="math inline">\(\nabla f\in S\)</span>，就有：</p>
<p><span class="math display">\[\begin{align}
\label{LFD-condition}
\text{对任何}\;\;d\in\text{LFD 都有 }\;\;d^{\mathrm T}[\nabla
f(x^\ast)]\geqslant0\quad\Rightarrow\quad [\nabla f(x^\ast)]\in S_+.
\end{align}\]</span></p>
<p>那么结合 (<span class="math inline">\(\ref{SFD-condition}\)</span>,
<span
class="math inline">\(\ref{LFD-condition}\)</span>)，我们就能得到以下一阶必要条件
(KKT 定理)：</p>
<blockquote class="colorquote theorem" ><p>如果 <span
class="math inline">\(\mathrm{SFD}=\mathrm{LFD}\)</span>，且 <span
class="math inline">\(x^\ast\)</span> 是局部极小值点，那么必然存在一组
<span class="math inline">\(\{\lambda_i\}_{i\in \mathcal E}\)</span> 与
<span class="math inline">\(\{\lambda_j\}_{j\in \mathcal
I}\)</span>，使得： <span class="math display">\[\begin{align*}
\nabla f(x^\ast)=\sum_{i\in\mathcal E}\lambda_i\nabla
E_i(x^\ast)+\sum_{j\in\mathcal I}\lambda_j\nabla I_j(x^\ast),\quad
\lambda_j\geqslant0\;(j\in\mathcal I),
\end{align*}\]</span></p>
</blockquote>
<p>所以，问题就归结于 <span
class="math inline">\(\mathrm{SFD}=\mathrm{LFD}\)</span>
何时能够成立。常见的情形有：</p>
<ul>
<li><span class="math inline">\(\{E_i\}_{i\in\mathcal
E}\cup\{I_j\}_{j\in\mathcal I}\)</span> 都是线性函数。</li>
<li>LICQ：<span class="math inline">\(\{\nabla E_i\}_{i\in\mathcal
E}\cup\{\nabla I_j\}_{j\in\mathcal I}\)</span> 线性无关。</li>
<li>Mangasarian-Fromowitz 条件：略(</li>
</ul>
<hr />
<h1 id="两种迭代法的思路">两种迭代法的思路</h1>
<p>求解优化问题的数值方法总是以迭代法为基础，即根据上一个点 <span
class="math inline">\(x_k\)</span> 的信息计算出下一个需要考虑的点 <span
class="math inline">\(x_{k+1}\)</span>，直到某一步的 <span
class="math inline">\(x_k\)</span>
达到收敛条件。迭代法的基本思路有两个：第一个思路是线搜索方法，第二个是信赖域方法。</p>
<p>线搜索方法在得到 <span class="math inline">\(x_k\)</span>
之后，先通过某种方法确定 <span class="math inline">\(x_k\)</span>
处的搜索方向 <span
class="math inline">\(p_k\)</span>，然后在这个方向上通过某种方法确定步长
<span class="math inline">\(\alpha_k\)</span>，就可得到下一个迭代点
<span class="math inline">\(x_{k+1}=x_k+\alpha_k p_k\)</span>。</p>
<p>信赖域方法用某个函数 <span class="math inline">\(m_k(x)\)</span> 近似
<span class="math inline">\(f(x)\)</span>，并假定这一近似在 <span
class="math inline">\(\|x-x_k\|\leqslant\Delta_k\)</span>
——即信赖域——的范围内成立，然后求出 <span
class="math inline">\(m_k(x)\)</span> 在信赖域内的极小值点 <span
class="math inline">\(x_{k+1}\)</span>，以此作为下一个迭代点，并依照结果调整
<span class="math inline">\(\Delta_k\)</span>。</p>
<h2 id="非精确线搜索的判定条件">非精确线搜索的判定条件</h2>
<p>在线搜索方法中，如果明确了搜索方向为 <span
class="math inline">\(p_k\)</span>，就需要选取合适的步长 <span
class="math inline">\(\alpha\)</span>
来确定下一个迭代点。一种可行的方法是精确线搜索，即求出使得 <span
class="math inline">\(f(x_k+\alpha p_k)\)</span> 取最小值的 <span
class="math inline">\(\alpha\)</span> 来作为 <span
class="math inline">\(\alpha_k\)</span>，但这种精确线搜索有时并不划算，因此需要使用非精确线搜索。在非精确线搜索中，只要一个
<span class="math inline">\(\alpha\)</span>
能够满足某些判定条件就可用于确定迭代点。</p>
<p>一个常见的判定条件是 Wolfe 条件，它由 Armijo 条件和曲率条件构成：</p>
<blockquote class="colorquote definition" ><p><strong>Armijo 条件（充分下降条件）</strong>：步长 <span
class="math inline">\(\alpha\)</span> 应该满足 <span
class="math display">\[f(x_k+\alpha p_k)\leqslant f(x_k)+c_1\alpha
p_k^{\mathrm T}\nabla f(x_k):= l(\alpha),\]</span> 其中常数 <span
class="math inline">\(c_1\)</span> 满足 <span
class="math inline">\(0&lt;c_1&lt;1\)</span>。</p>
<p><strong>曲率条件</strong>：步长 <span
class="math inline">\(\alpha\)</span> 应该满足 <span
class="math display">\[p_k^{\mathrm T}\nabla f(x_k+\alpha p_k)\geqslant
c_2p_k^{\mathrm T}\nabla f(x_k),\]</span> 其中常数 <span
class="math inline">\(c_2\)</span> 满足 <span
class="math inline">\(c_1&lt;c_2&lt;1\)</span>。</p>
</blockquote>
<p>这两个条件的含义如下：</p>
<ul>
<li><p>Armijo 条件：步长 <span class="math inline">\(\alpha\)</span>
应该使得 <span class="math inline">\(f(x)\)</span>
充分下降，如下图所示。</p></li>
<li><p>曲率条件的含义如下图所示：不等式左侧为 <span
class="math inline">\(\mathrm d f(x_k+\alpha p_k)/\mathrm
d\alpha\)</span>，不等式右侧为 <span
class="math inline">\(\alpha=0\)</span> 处的 <span
class="math inline">\(\mathrm d f(x_k+\alpha p_k)/\mathrm
d\alpha\)</span>，后者是负的，前者应该比后者更大一些，以便更加接近极小值点。</p></li>
</ul>
<center>
<p><img src="/picture/Wolfe_condition.png" width="75%"></p>
<div
style="display: inline-block; width: 75%; text-align: left; color: #999999; font-size: 8">
<p>Wolfe 条件示意图。 斜率 desired slope 就是 <span
class="math inline">\(f(x_k+\alpha p_k)\)</span> 对 <span
class="math inline">\(\alpha\)</span> 的导数。斜线 <span
class="math inline">\(l(\alpha)\)</span> 是 <span
class="math inline">\(l(\alpha):= f(x_k)+c_1\alpha p_k^{\mathrm T}\nabla
f(x_k)\)</span>。区间 acceptable 就是同时满足这两个条件的 <span
class="math inline">\(\alpha\)</span>：既要让 <span
class="math inline">\(f(x_k+\alpha p_k)\)</span> 位于 <span
class="math inline">\(l(\alpha)\)</span> 下方，又要让 <span
class="math inline">\(f(x_k+\alpha p_k)\)</span> 的斜率相较 <span
class="math inline">\(p_k^{\mathrm T}\nabla f(x_k)\)</span>
更大一些。</p>
</div>
</center>
<p>可以看到，曲率条件也会筛选出那些使得 <span
class="math inline">\(\mathrm d f(x_k+\alpha p_k)/\mathrm
d\alpha&gt;0\)</span> 的 <span
class="math inline">\(\alpha\)</span>。<strong>强 Wolfe 条件</strong>
将曲率条件修改为 <span class="math display">\[\left| p_k^{\mathrm
T}\nabla f(x_k+\alpha p_k)\right| \geqslant\left|  c_2p_k^{\mathrm
T}\nabla f(x_k)\right|,\]</span> 以便将这些 <span
class="math inline">\(\alpha\)</span> 排除出去。</p>
<h2 id="信赖域方法">信赖域方法</h2>
<h3 id="一般思路">一般思路</h3>
<p>信赖域方法就是用一个函数 <span class="math inline">\(m_k\)</span>
来近似第 <span class="math inline">\(k\)</span> 步迭代点 <span
class="math inline">\(x_k\)</span> 附近的 <span
class="math inline">\(f(x)\)</span>，并假设这一近似在 <span
class="math inline">\(\|x-x_k\|&lt;\Delta_k\)</span>
的范围内（即信赖域以内）成立。通常将 <span
class="math inline">\(m_k\)</span> 取为二次函数： <span
class="math display">\[\begin{align}\label{trust-region-problem}
m_k(x)=\dfrac12(x-x_k)^{\mathrm T}G_k(x-x_k)-b_k^{\mathrm
T}(x-x_k)+(\cdots),\quad \|x-x_k\|\leqslant\Delta_k.
\end{align}\]</span></p>
<p>信赖域方法通常会通过某种方式在信赖域内部找出下一个迭代点 <span
class="math inline">\(x_{k+1}\)</span> 使它满足 <span
class="math inline">\(m_k(x_{k+1})&lt;m_k(x_k)\)</span>，并计算 <span
class="math display">\[\begin{align*}
\rho_k:=\dfrac{f(x_k)-f(x_{k+1})}{m_k(x_k)-m_k(x_{k+1})},
\end{align*}\]</span> 然后根据 <span
class="math inline">\(\rho_k\)</span>
的大小确定下一步的要做的两件事情：</p>
<blockquote class="colorquote algorithm" ><ul>
<li><p>调整信赖域大小：</p>
<ul>
<li><p>如果 <span class="math inline">\(\rho_k&lt;1/4\)</span>：这说明
<span class="math inline">\(m_k(x)\)</span> 并不能很好地近似 <span
class="math inline">\(f(x)\)</span>，下一步需要缩小信赖域的范围，例如取
<span
class="math inline">\(\Delta_{k+1}=\Delta_{k}/4\)</span>。</p></li>
<li><p>如果 <span
class="math inline">\(1/4&lt;\rho_k&lt;3/4\)</span>：那么 <span
class="math inline">\(m_k(x)\)</span> 作为 <span
class="math inline">\(f(x)\)</span>
的近似不好不坏，信赖域在下一步迭代时不变：<span
class="math inline">\(\Delta_{k+1}=\Delta_{k}\)</span>。</p></li>
<li><p>如果 <span class="math inline">\(\rho_k&gt;3/4\)</span>：那么
<span class="math inline">\(m_k(x)\)</span> 是 <span
class="math inline">\(f(x)\)</span>
的很好的近似，在下一步迭代时可以适当扩大信赖域的范围，例如取 <span
class="math inline">\(\Delta_{k+1}=2\Delta_{k}\)</span>。</p></li>
</ul></li>
<li><p>选取下一个迭代点：</p>
<ul>
<li><p>如果 <span class="math inline">\(\rho_k&lt;\eta\)</span>（<span
class="math inline">\(\eta\)</span> 为某个位于 <span
class="math inline">\(0\sim1/4\)</span> 间的常数），这一步迭代并没有让
<span class="math inline">\(f(x)\)</span> 发生足够的下降甚至让 <span
class="math inline">\(f(x)\)</span> 发生了上升，因此 <span
class="math inline">\(x_{k+1}\)</span>
不能作为下一个迭代点。此时需要做回退 <span
class="math inline">\(x_{k+1}=x_k\)</span> 并在调整信赖域大小后重新计算
<span class="math inline">\(x_{k+1}\)</span>。</p></li>
<li><p>如果 <span class="math inline">\(\rho_k&gt;\eta\)</span>
那么就可以接受 <span class="math inline">\(x_{k+1}\)</span>
作为新的迭代点。</p></li>
</ul></li>
</ul>
</blockquote>
<p><span id="Levenberg-Marquardt"></p>
<h3
id="另一种思路levenberg-marquardt-方法">另一种思路（Levenberg-Marquardt
方法）</h3>
<p></span></p>
<p>最小化 (<span
class="math inline">\(\ref{trust-region-problem}\)</span>)
可被视为是一个约束优化问题。如果将信赖域取为 <span
class="math inline">\(\|x-x_k\|_2^2\leqslant\Delta_k^2\)</span>，那么这一问题的
KKT 条件为： <span
class="math display">\[\begin{align}\label{LM-method-KKT-condition}
(G_k+\lambda_k I)(x-x_k)=b_k,\quad \lambda_k\geqslant0,\quad
\lambda_k(\|x-x_k\|_2-\Delta_k)=0.
\end{align}\]</span></p>
<p>如果 <span class="math inline">\(G\)</span>
是对称正定的，那么就有一种求解这一约束优化问题的有趣思路。容易看出（通过矩阵对角化），<span
class="math inline">\(\lambda_k\)</span> 越大，求解 (<span
class="math inline">\(\ref{LM-method-KKT-condition}\)</span>)
的第一个式子给出的 <span class="math inline">\(\|x-x_k\|_2\)</span>
就越小，因此，<span class="math inline">\(\lambda_k\)</span>
也可以起到类似于 <span class="math inline">\(\Delta_k\)</span>
的作用。</p>
<p>于是，我们有如下魔改版的信赖域方法：</p>
<blockquote class="colorquote algorithm" ><ul>
<li><p>选定一个 <span class="math inline">\(\lambda_k\)</span>，求解
(<span class="math inline">\(\ref{LM-method-KKT-condition}\)</span>)
的第一个式子得到 <span class="math inline">\(x_{k+1}\)</span>，并计算
<span class="math inline">\(\rho_k\)</span>。</p>
<ul>
<li><p>如果 <span
class="math inline">\(\rho_k&lt;1/4\)</span>：在下一步中选取 <span
class="math inline">\(\lambda_{k+1}=4\lambda_k\)</span>
达到缩小信赖域的效果。</p></li>
<li><p>如果 <span
class="math inline">\(1/4&lt;\rho_k&lt;3/4\)</span>：在下一步中选取
<span class="math inline">\(\lambda_{k+1}=\lambda_k\)</span>。</p></li>
<li><p>如果 <span
class="math inline">\(\rho_k&gt;3/4\)</span>：在下一步中选取 <span
class="math inline">\(\lambda_{k+1}=\lambda_k/2\)</span>
达到扩大信赖域的效果。</p></li>
</ul></li>
<li><p>选取下一个迭代点：同一般思路</p></li>
</ul>
</blockquote>
<hr />
<h1 id="无约束优化问题的拟牛顿法">无约束优化问题的拟牛顿法</h1>
<h2 id="牛顿法的基本思路">牛顿法的基本思路</h2>
<p>牛顿法的基本思路是：对 <span class="math inline">\(\nabla
f(x)\)</span> 做以下近似： <span class="math display">\[\begin{align*}
\nabla f(x)\approx \nabla f(x_k)+[\nabla^2f(x_k)] (x-x_k),
\end{align*}\]</span> 然后通过 <span class="math inline">\(\nabla
f(x_{k+1})=0\)</span> 的条件将下一步的迭代点选为：</p>
<blockquote class="colorquote theorem" ><p><span class="math display">\[\begin{align}\label{Newton-method}
x_{k+1}=x_k-\alpha_k[\nabla^2f(x_k)]^{-1}\nabla f(x_k).
\end{align}\]</span></p>
</blockquote>
<p><span class="math inline">\(\alpha_k=1\)</span> 或由线搜索确定。</p>
<p>搜索方向 <span class="math inline">\(d_k=-[\nabla^2f(x_k)]^{-1}\nabla
f(x_k)\)</span> 通常由求解如下线性方程组确定： <span
class="math display">\[\begin{align*}
[\nabla^2f(x_k)]d=-\nabla f(x_k).
\end{align*}\]</span>
在问题规模较大时，严格求解这一线性方程组往往是很困难的，并且还可能是没有必要的，因此我们可以借用其他方法近似求解这一线性方程组。
在 python 的 scipy.optimize.minimize 函数中，Newton-CG
方法的思想就是用做了某些小修正的共轭梯度法求解这一方程组，trust-ncg
方法可被视为是结合了信赖域方法的 Newton-CG 方法，而 trust-krylov 与
trust-exact 则使用了其他方法求解信赖域方法中需要用到的步长。</p>
<h2 id="拟牛顿法的基本思路">拟牛顿法的基本思路</h2>
<p>牛顿法需要计算二阶导，代价比较大。拟牛顿法的思路就是用一系列比较好算的矩阵
<span class="math inline">\(B_k\)</span> 来近似二阶导 <span
class="math inline">\([\nabla^2f(x_k)]\)</span>，或者用一系列比较好算的矩阵
<span class="math inline">\(H_k\)</span> 来近似二阶导 <span
class="math inline">\([\nabla^2f(x_k)]^{-1}\)</span>。</p>
<p>拟牛顿法在最开始做迭代时，采用 <span
class="math inline">\([\nabla^2f(x_0)]\)</span> 或 <span
class="math inline">\([\nabla^2f(x_0)]^{-1}\)</span> 来作为 <span
class="math inline">\(B_0\)</span> 或 <span
class="math inline">\(H_0\)</span>；在从 <span
class="math inline">\(x_k\)</span> 得到 <span
class="math inline">\(x_{k+1}\)</span> 之后，对 <span
class="math inline">\(B_k\)</span> 或 <span
class="math inline">\(H_k\)</span> 做出修正得到 <span
class="math inline">\(B_{k+1}\)</span> 或 <span
class="math inline">\(H_{k+1}\)</span> 用于下一步的迭代。要让 <span
class="math inline">\(B_k\)</span> 或 <span
class="math inline">\(H_k\)</span> 一直成为 <span
class="math inline">\([\nabla^2f(x_k)]\)</span> 或 <span
class="math inline">\([\nabla^2f(x_k)]^{-1}\)</span>
的好的近似，就需要让它们满足拟牛顿条件。</p>
<p>我们定义： <span class="math display">\[\begin{align*}
y_k:= \nabla f(x_{k+1})-\nabla f(x_k),\quad s_k:= x_{k+1} - x_k.
\end{align*}\]</span> 显然，二阶导数近似地满足： <span
class="math display">\[\begin{align*}
\nabla f(x_k)\approx \nabla f(x_{k+1})+[\nabla^2f(x_{k+1})]
(x_k-x_{k+1}),\quad \text{或}\quad [\nabla^2f(x_{k+1})]s_k=y_k.
\end{align*}\]</span> 拟牛顿条件就是：</p>
<blockquote class="colorquote theorem" ><p><span class="math display">\[\begin{align}\label{secant condition}
B_{k+1}s_k=y_k,\quad \text{或}\quad s_k=H_{k+1}y_k.
\end{align}\]</span></p>
</blockquote>
<h2 id="三种常用的修正">三种常用的修正</h2>
<h3 id="sr1-修正">SR1 修正</h3>
<p>最简单的对 <span class="math inline">\(B_k\)</span> 或 <span
class="math inline">\(H_k\)</span> 做出修正得到 <span
class="math inline">\(B_{k+1}\)</span> 或 <span
class="math inline">\(H_{k+1}\)</span> 的方法是 SR1（对称秩
1）修正。其思路如下：假设 <span class="math display">\[\begin{align*}
B_{k+1}=B_k+u_ku_k^{\mathrm T}.
\end{align*}\]</span> <span class="math inline">\(B_{k+1}\)</span> 与
<span class="math inline">\(B_k\)</span> 只相差一个秩为 <span
class="math inline">\(1\)</span>
的对称矩阵，这就是这一方法的名称的来源。于是由拟牛顿条件 (<span
class="math inline">\(\ref{secant condition}\)</span>) 可以得到： <span
class="math display">\[\begin{align*}
u_k(u_k^{\mathrm T}s_k)+B_ks_k=y_k.
\end{align*}\]</span> 注意到 <span class="math inline">\((u_k^{\mathrm
T}s_k)\)</span> 是常数，所以 <span class="math inline">\(u_k\)</span>
的形式必然为 <span
class="math inline">\(u_k=a(y_k-B_ks_k)\)</span>。代入以上表达式就不难算出
<span class="math inline">\(a^2=1/[(y_k-B_ks_k)^{\mathrm
T}s_k]\)</span>，这样就得到了 <span
class="math inline">\(B_{k+1}\)</span>。<span
class="math inline">\(H_{k+1}\)</span> 可通过一样的方法得到。</p>
<p>SR1 修正的最终表达式为：</p>
<blockquote class="colorquote theorem" ><p><span class="math display">\[\begin{align*}
\begin{split}
\text{SR1}:\quad B_{k+1}=&amp;\;B_k+\dfrac{1}{(y_k-B_ks_k)^{\mathrm
T}s_k}(y_k-B_ks_k)(y_k-B_ks_k)^{\mathrm T},\\
\text{SR1}:\quad H_{k+1}=&amp;\;H_k+\dfrac{1}{(s_k-H_ky_k)^{\mathrm
T}y_k}(s_k-H_ky_k)(s_k-H_ky_k)^{\mathrm T}.
\end{split}
\end{align*}\]</span></p>
</blockquote>
<p>SR1 方法的问题在于，并不能保证 <span
class="math inline">\(B_{k+1}\)</span> 或 <span
class="math inline">\(H_{k+1}\)</span> 的正定性。</p>
<h3 id="bfgs-与-dfp-修正">BFGS 与 DFP 修正</h3>
<p>另两套重要的修正方法是 BFGS 与 DFP 修正。BFGS 方法的思路如下：假设
<span class="math display">\[\begin{align*}
B_{k+1}=B_k+u_ku_k^{\mathrm T}+v_kv_k^{\mathrm T}.
\end{align*}\]</span> 由拟牛顿条件可得： <span
class="math display">\[\begin{align*}
y_k=B_ks_k+u_k(u_k^{\mathrm T}s_k)+v_k(v_k^{\mathrm T}s_k).
\end{align*}\]</span> 假设 <span
class="math inline">\(u_k=aB_ks_k\)</span>、<span
class="math inline">\(v_k=b y_k\)</span>，不难算出 <span
class="math inline">\(a^2=-1/[(B_ks_k)^{\mathrm T}s_k]\)</span>、<span
class="math inline">\(b^2=1/(y_k^{\mathrm T}s_k)\)</span>，这样就得到了
BFGS 修正。</p>
<p>DFP 修正就是对 <span class="math inline">\(H_k\)</span>
做同样的流程，最终的结果是：</p>
<blockquote class="colorquote theorem" ><p><span class="math display">\[\begin{align*}
\begin{split}
\text{BFGS}:\quad&amp;B_{k+1}=B_k-\dfrac{1}{(B_ks_k)^{\mathrm
T}s_k}(B_ks_k)(B_ks_k)^{\mathrm T}+\dfrac{1}{y_k^{\mathrm
T}s_k}y_ky_k^{\mathrm T},\\
\text{DFP}:\quad&amp;H_{k+1}=H_k-\dfrac{1}{(H_ky_k)^{\mathrm
T}y_k}(H_ky_k)(H_ky_k)^{\mathrm T}+\dfrac{1}{s_k^{\mathrm
T}y_k}s_ks_k^{\mathrm T}.
\end{split}
\end{align*}\]</span></p>
</blockquote>
<p>在处理大规模问题时，每次都重新计算 <span
class="math inline">\(B_k\)</span>
是不合适的，一个可取的方法是记录下最近的若干个 <span
class="math inline">\(u_k\)</span> 与 <span
class="math inline">\(v_k\)</span>，这样就不需要再计算 <span
class="math inline">\(B_k\)</span>。这就是所谓的有限内存 (limited
memory) BFGS (L-BFGS) 法的大致思路。</p>
<h1 id="无约束优化问题的共轭梯度法">无约束优化问题的共轭梯度法</h1>
<h2 id="二次函数的共轭梯度法">二次函数的共轭梯度法</h2>
<p>现在，考虑二次函数的无约束优化问题： <span
class="math display">\[\begin{align}\label{quadratic-problem}
f(x)=\dfrac12 x^{\mathrm T}Gx-b^{\mathrm T}x,
\end{align}\]</span> 其中 <span class="math inline">\(G\)</span>
为对称正定矩阵。</p>
<h3 id="基本思路">基本思路</h3>
<p>我们可以指定 <span class="math inline">\(n\)</span>
个线性无关的方向（<span class="math inline">\(n\)</span> 为 <span
class="math inline">\(x\)</span> 的维数）<span
class="math inline">\(p_{1\sim n}\)</span>，使其满足： <span
class="math display">\[\begin{align*}
p^{\mathrm T}_iGp_j=\delta_{ij}(p^{\mathrm T}_iGp_i),
\end{align*}\]</span> 很容易看出来，<span
class="math inline">\(p_i\)</span> 线性无关，且构成了 <span
class="math inline">\(\mathbb R^n\)</span> 的一组 <span
class="math inline">\(G\)</span>-正交基——如果将矢量内积 <span
class="math inline">\(\langle x,y\rangle\)</span> 定义为 <span
class="math inline">\(x^{\mathrm T}Gy\)</span> 的话。</p>
<p>由于 <span class="math inline">\(p_i\)</span> 是一组基，所以最小化
(<span class="math inline">\(\ref{quadratic-problem}\)</span>)
时选取的初值 <span class="math inline">\(x_0\)</span> 与严格最小值 <span
class="math inline">\(x^\ast\)</span> 之间的差异一定可以用 <span
class="math inline">\(p_i\)</span> 线性表示出来： <span
class="math display">\[\begin{align*}
x_0=x^\ast-\sum_{i=0}^{n-1} \alpha_ip_i.
\end{align*}\]</span> 共轭梯度法生成如下序列： <span
class="math display">\[\begin{align}\label{CG x_k}
x_k=x^\ast-\sum_{i=k}^{n-1} \alpha_ip_i=x_{k-1}+\alpha_{k-1}p_{k-1},
\end{align}\]</span> 使得 <span class="math inline">\(n\)</span>
步迭代后的 <span class="math inline">\(x_{n}\)</span> 就等于 <span
class="math inline">\(x^\ast\)</span>。也就是说，共轭梯度法在第 <span
class="math inline">\(k\)</span> 步迭代的时候会消除 <span
class="math inline">\(x_0\)</span> 与 <span
class="math inline">\(x^\ast\)</span> 在 <span
class="math inline">\(p_k\)</span> 方向上的差异，遍历完全部的 <span
class="math inline">\(n\)</span> 个方向之后，<span
class="math inline">\(x_n\)</span> 自然就会等于 <span
class="math inline">\(x^\ast\)</span>。</p>
<p>共轭梯度法并不是在一开始就确定了全部的 <span
class="math inline">\(p_k\)</span>，<span
class="math inline">\(p_k\)</span> 的具体形式需要在计算出 <span
class="math inline">\(x_{0\sim k}\)</span>，<span
class="math inline">\(p_{0\sim (k-1)}\)</span> 以及 <span
class="math inline">\(r_{0\sim k}\)</span> 之后才能确定（因为 <span
class="math inline">\(p_k\)</span> 是用于从 <span
class="math inline">\(x_k\)</span> 得到 <span
class="math inline">\(x_{k+1}\)</span> 的），其中 <span
class="math inline">\(r_k\)</span> 表示 <span
class="math inline">\(x_k\)</span> 处的梯度： <span
class="math display">\[\begin{align*}
r_k:= Gx_k-b.
\end{align*}\]</span></p>
<h3 id="p_k-的选取"><span class="math inline">\(p_k\)</span> 的选取</h3>
<p>以下我们就来讨论 <span class="math inline">\(p_k\)</span>
的选取方式。如果 <span class="math inline">\(r_k\)</span> 在某一步变为
<span class="math inline">\(0\)</span>
就意味着我们已经找到了原二次函数的极值点。如果不为 <span
class="math inline">\(0\)</span>，那么，由 (<span
class="math inline">\(\ref{CG x_k}\)</span>) 以及 <span
class="math inline">\(Gx^\ast=b\)</span> 可以发现 <span
class="math display">\[\begin{align}\label{CG r_k}
r_k=\left(Gx^\ast-\sum_{i=k}^{n-1}
\alpha_iGp_i\right)-b=-\sum_{i=k}^{n-1} \alpha_iGp_i.
\end{align}\]</span></p>
<p>由于 <span class="math inline">\(p_i\)</span> 应该是 <span
class="math inline">\(G\)</span>-正交的，所以 <span
class="math inline">\(p_{0\sim(k-1)}^{\mathrm T}r_k=0\)</span>，这意味着
<span class="math inline">\(r_k\)</span> 中必定包含了独立于 <span
class="math inline">\(p_0\sim p_{k-1}\)</span>
以外的方向，于是我们可以从 <span class="math inline">\(r_k\)</span>
中提取出一个方向作为 <span class="math inline">\(p_k\)</span>。</p>
<p>我们假定 <span class="math inline">\(p_k\)</span> 具有如下形式：
<span class="math display">\[\begin{align}\label{p_k-form}
p_k=-r_k+\beta_kp_{k-1}+\sum_{j=0}^{k-2}\gamma_{k,j}p_j.
\end{align}\]</span> 系数 <span
class="math inline">\(\beta_k,\gamma_{k,j}\)</span> 应当使 <span
class="math inline">\(p^{\mathrm
T}_{0\sim(k-1)}Gp_k=0\)</span>。可以证明， 符合这一条件的系数为： <span
class="math display">\[\gamma_{k,j}=0,\]</span> 以及 <span
class="math display">\[\begin{align}\label{CG-beta_k}
\beta_k=\dfrac{r_k^{\mathrm T}(r_k-r_{k-1})}{p_{k-1}^{\mathrm
T}(r_k-r_{k-1})}=\dfrac{r_k^{\mathrm T}(r_k-r_{k-1})\quad \text{或}\quad
r_k^{\mathrm T}r_k}{p_{k-1}^{\mathrm T}(r_k-r_{k-1})\quad \text{或}\quad
(-p_{k-1}^{\mathrm T}r_{k-1})\quad\text{或}\quad r_{k-1}^{\mathrm
T}r_{k-1}},
\end{align}\]</span> 此外，这样定出的 <span
class="math inline">\(p_k\)</span> 满足： <span
class="math display">\[p_{i=0\sim(k-1)}^{\mathrm T}r_k=0,\quad
r_{i=0\sim(k-1)}^{\mathrm T}r_k=0.\]</span></p>
<div class='spoiler collapsed'>
    <div class='spoiler-title'>
        证明
    </div>
    <div class='spoiler-content'>
        <p>首先证明 <span class="math inline">\(\gamma_{k,j}\)</span> 全部为
<span class="math inline">\(0\)</span>。</p>
<p>为此，我们在 (<span class="math inline">\(\ref{p_k-form}\)</span>)
式两侧同时左乘 <span class="math inline">\(p_j^{\mathrm T}G\)</span>：
<span class="math display">\[\begin{align*}
(p_j^{\mathrm T}Gp_j)\gamma_{k,j}=&amp;\;p_j^{\mathrm
T}Gp_k+p_j^{\mathrm T}Gr_k-\beta p_j^{\mathrm T}Gp_{k-1}=p_j^{\mathrm
T}Gr_k\\
=&amp;\;\dfrac{1}{\alpha_j}(x_{j+1}-x_j)^{\mathrm T}Gr_k\\
=&amp;\;\dfrac{1}{\alpha_j}(r_{j+1}-r_j)^{\mathrm T}r_k.
\end{align*}\]</span> 其中 <span
class="math inline">\(j=0\sim(k-2)\)</span>。不难看出，只需证明 <span
class="math inline">\(r_{i=0\sim(k-1)}^{\mathrm T}r_k=0\)</span>
就能完成证明。</p>
<p>(<span class="math inline">\(\ref{CG r_k}\)</span>) 告诉我们 <span
class="math inline">\(r_k\)</span> 是 <span
class="math inline">\(Gp_{k\sim(n-1)}\)</span> 的线性组合，因此如果
<span class="math inline">\(i&lt;k\)</span> 就一定有： <span
class="math display">\[p_{i=0\sim(k-1)}^{\mathrm T}r_k=0.\]</span></p>
<p>从我们选取的 (<span class="math inline">\(\ref{p_k-form}\)</span>)
可以看出，<span class="math inline">\(r_i\)</span> 是 <span
class="math inline">\(p_{0\sim i}\)</span> 的线性组合，因此： <span
class="math display">\[r_{i=0\sim(k-1)}^{\mathrm T}r_k=0.\]</span>
这样就完成了证明。</p>
<hr />
<p>由于 <span class="math inline">\(\gamma_{k,j}\)</span> 全部为 <span
class="math inline">\(0\)</span>，所以 <span
class="math inline">\(p_k\)</span> 的形式为： <span
class="math display">\[\begin{align*}
p_k=-r_k+\beta_kp_{k-1}.
\end{align*}\]</span> 再结合 <span
class="math inline">\(p_{k-1}^{\mathrm T}Gp_k=0\)</span> 的条件可以算出
<span class="math inline">\(\beta_k\)</span>： <span
class="math display">\[\begin{align*}
\beta_k=\dfrac{p^{\mathrm T}_{k-1}Gr_k}{p_{k-1}^{\mathrm
T}Gp_{k-1}}=\dfrac{r_k^{\mathrm T}Gp_{k-1}}{p_{k-1}^{\mathrm
T}Gp_{k-1}}.
\end{align*}\]</span> 从 (<span class="math inline">\(\ref{CG
r_k}\)</span>) 可以看出 <span
class="math inline">\(r_k=r_{k-1}+\alpha_{k-1}Gp_{k-1}\)</span>，所以
<span
class="math inline">\(Gp_{k-1}\propto(r_k-r_{k-1})\)</span>，因此以上表达式也可以写为：
<span class="math display">\[\begin{align*}
\beta_k=\dfrac{r_k^{\mathrm T}(r_k-r_{k-1})}{p_{k-1}^{\mathrm
T}(r_k-r_{k-1})}=\dfrac{r_k^{\mathrm T}(r_k-r_{k-1})\quad \text{或}\quad
r_k^{\mathrm T}r_k}{p_{k-1}^{\mathrm T}(r_k-r_{k-1})\quad \text{或}\quad
(-p_{k-1}^{\mathrm T}r_{k-1})\quad\text{或}\quad r_{k-1}^{\mathrm
T}r_{k-1}},
\end{align*}\]</span> 第二个等号用到了 <span
class="math inline">\(r_k^{\mathrm T}r_{0\sim(k-1)}=0\)</span>、<span
class="math inline">\(p_{0\sim(k-1)}^{\mathrm T}r_k=0\)</span>
等各类结果。这一结果的重要之处在于：<span
class="math inline">\(\beta_k\)</span> 的表达式中可以摆脱二阶导 <span
class="math inline">\(G\)</span>。</p>

    </div>
</div>
<h3 id="alpha_k-的选取"><span class="math inline">\(\alpha_k\)</span>
的选取</h3>
<p>在明确了 <span class="math inline">\(\beta_k\)</span>
之后，我们可以考虑 <span class="math inline">\(f(x_k+\alpha
p_k)\)</span>： <span class="math display">\[\begin{align*}
f(x_k+\alpha p_k)=\dfrac12p_k^{\mathrm T}Gp_k\alpha^2+p_k^{\mathrm
T}(Gx_k-b)\alpha+(\cdots),
\end{align*}\]</span> <span class="math inline">\(\alpha_k\)</span>
应该让 <span class="math inline">\(f(x_k+\alpha p_k)\)</span>
取极小值以便消除 <span class="math inline">\(x_k\)</span> 与 <span
class="math inline">\(x^\ast\)</span> 在 <span
class="math inline">\(p_k\)</span> 方向上的差距： <span
class="math display">\[\begin{align}\label{CG alpha_k}
\alpha_k=-\dfrac{p_k^{\mathrm T}r_k}{p_k^{\mathrm T}Gp_k}.
\end{align}\]</span></p>
<p>总结一下，二次函数的共轭梯度法的流程大致为： <span
class="math display">\[x_k\rightarrow r_k\rightarrow p_k\rightarrow
\alpha_k\rightarrow x_{k+1}(=x_k+\alpha_kp_k).\]</span></p>
<h2 id="非二次函数的共轭梯度法">非二次函数的共轭梯度法</h2>
<p>非二次函数的共轭梯度法直接使用 (<span
class="math inline">\(\ref{p_k-form}\)</span>, <span
class="math inline">\(\ref{CG-beta_k}\)</span>) 算出 <span
class="math inline">\(p_k\)</span> 作为搜索方向（<span
class="math inline">\(\gamma_{k,j}=0\)</span>，<span
class="math inline">\(r_k\)</span> 就直接取为 <span
class="math inline">\(x_k\)</span> 的导数），然后在 <span
class="math inline">\(p_k\)</span>
方向做线搜索进而得到下一个迭代点。</p>
<hr />
<h1 id="其他问题与算法">其他问题与算法</h1>
<h2 id="最小二乘问题">最小二乘问题</h2>
<h3 id="一般的最小二乘问题">一般的最小二乘问题</h3>
<p>最小二乘问题的被优化函数 <span class="math inline">\(f(x)\)</span>
一般具有如下形式： <span class="math display">\[\begin{align*}
f(x)=\dfrac12\sum_{i=1}^m [r_i(x)]^2,\quad m&gt;n\;(n\text{ 为
}\;x\text{ 的维数}).
\end{align*}\]</span> 我们可以将 <span class="math inline">\(r_{i=1\sim
m}\)</span> 排成列向量，我们还可以定义一个 <span
class="math inline">\(m\)</span> 行 <span
class="math inline">\(n\)</span> 列的矩阵 <span
class="math inline">\(J\)</span>，使其第 <span
class="math inline">\(i\)</span> 行就是排成行向量的 <span
class="math inline">\(\nabla r_i(x)\)</span>： <span
class="math display">\[\begin{align*}
r(x):=[r_1,r_2,\cdots,r_m]^{\mathrm T},\quad J(x):=[\nabla r_1,\nabla
r_2,\cdots,\nabla r_m]^{\mathrm T}.
\end{align*}\]</span> 不难看出，<span
class="math inline">\(f(x)\)</span> 的一阶导和二阶导分别为： <span
class="math display">\[\begin{align*}
\nabla f(x) = J(x)^{\mathrm T}r(x),\quad \nabla^2f(x)=J(x)^{\mathrm
T}J(x)+\sum_{i=1}^mr_i(x)\nabla^2r_i(x).
\end{align*}\]</span></p>
<p>线性最小二乘问题可以用牛顿法处理。在牛顿法的流程中，如果近似地认为
<span class="math display">\[\begin{align*}
\nabla^2f(x)\approx J^{\mathrm T}(x)J(x),
\end{align*}\]</span> 也就是忽略 <span class="math inline">\(\nabla
f(x)\)</span> 中的第二项，就得到了高斯-牛顿法。这一方法非常适用于 <span
class="math inline">\(r_i(x)\)</span>
都比较小或都比较接近线性函数的情形。</p>
<p>另一种重要的方法是 Levenberg-Marquardt 方法，其基本思路就是 <a
href="#Levenberg-Marquardt">Levenberg-Marquardt 方法小节</a>
所描述的思路。事实上，Levenberg-Marquardt
方法可被视为是信赖域方法的起源之一。稍加改进的 Levenberg-Marquardt
方法是现在处理无约束最小二乘问题的最优方法。</p>
<h3 id="线性最小二乘问题">线性最小二乘问题</h3>
<p>最简单的最小二乘问题就是线性最小二乘问题，这里的被优化函数的形式是：
<span class="math display">\[\begin{align*}
f(x)=\dfrac12(Jx-y)^{\mathrm T}(Jx-y).
\end{align*}\]</span></p>
<p>我们可以用 QR 分解的思路处理线性最小二乘问题。假设 <span
class="math inline">\(J\)</span> 的 QR 分解为： <span
class="math display">\[\begin{align*}
J=Q_{m\times m}\begin{bmatrix}R_{n\times n}\\0_{(m-n)\times
n}\end{bmatrix}=\begin{bmatrix}(Q_1)_{m\times n}&amp;(Q_2)_{m\times
(m-n)}\end{bmatrix}\begin{bmatrix}R_{n\times n}\\0_{(m-n)\times
n}\end{bmatrix}
\end{align*}\]</span> 于是： <span class="math display">\[\begin{align*}
f(x)=\dfrac12(Q^{\mathrm T}Jx-Q^{\mathrm T}y)^{\mathrm T}(Q^{\mathrm
T}Jx-Q^{\mathrm T}y)=\dfrac12\|Rx-Q_1^{\mathrm
T}y\|^2+\dfrac12\|Q_2^{\mathrm T}y\|^2.
\end{align*}\]</span>
这意味着，线性最小二乘问题的解就是如下线性方程组的解： <span
class="math display">\[\begin{align*}
Rx=Q_1^{\mathrm T}y.
\end{align*}\]</span></p>
<p>我们也可以在以上方程组两边同时乘以 <span
class="math inline">\(R^{\mathrm T}\)</span>。注意到 <span
class="math inline">\(R^{\mathrm T}R=J^{\mathrm T}J\)</span>，<span
class="math inline">\(R^{\mathrm T}Q_1^{\mathrm T}=(Q_1R)^{\mathrm
T}=J^{\mathrm T}\)</span>，所以以上方程组也就等价于： <span
class="math display">\[\begin{align*}
J^{\mathrm T}Jx=J^{\mathrm T}y.
\end{align*}\]</span> <span class="math inline">\(J^{\mathrm
T}J\)</span> 总是对称半正定的，所以可通过 Cholesky
分解等方法求解这一线性方程组。</p>
<p>另一种思路是对 <span class="math inline">\(J\)</span> 做 SVD
分解，再参考 QR 分解的思路。</p>
<h2 id="线性规划问题的单纯形法">线性规划问题的单纯形法</h2>
<h3 id="线性规划问题">线性规划问题</h3>
<p>优化目标与约束条件都是 <span class="math inline">\(x\)</span>
的线性函数的优化问题就是线性规划问题。线性规划问题的标准形式为； <span
class="math display">\[\begin{align}
\text{最小化}\;\;c^{\mathrm T}x,\qquad x\;\text{满足}\;
\begin{cases}
x\geqslant0\\
Ax=b
\end{cases}.
\end{align}\]</span> 其中 <span class="math inline">\(A\)</span> 为
<span class="math inline">\(m\times n\)</span> 的行满秩矩阵。</p>
<p>对于形如 <span class="math inline">\(a&#39;^{\mathrm T}x\geqslant
b&#39;\)</span> 这样的不等式约束，我们可以引入松弛变量 <span
class="math inline">\(x&#39;\geqslant0\)</span> 来使其变为 <span
class="math inline">\(a^{\mathrm T}x-x&#39;=b\)</span>。对于取值范围为
<span class="math inline">\(\mathbb R\)</span> 的自变量 <span
class="math inline">\(x&#39;&#39;\)</span>，我们可以引入两个变量 <span
class="math inline">\(x_1\geqslant0\)</span> 和 <span
class="math inline">\(x_2\geqslant0\)</span>，将 <span
class="math inline">\(x&#39;&#39;\)</span> 改写为 <span
class="math inline">\(x_1-x_2\)</span>。如果是最大化 <span
class="math inline">\(c^{\mathrm T}x\)</span>，那么可将优化目标选取为
<span class="math inline">\(-c^{\mathrm
T}x\)</span>。这样一来，所有线性规划问题都能被化为以上标准形式。</p>
<p>处理线性规划问题的最重要的算法是单纯形法。需要注意的是，除了线性规划的单纯性法以外，优化问题中还有另一种截然不同的、不需要计算导数的单纯形法。</p>
<h3 id="单纯形法的基本思路">单纯形法的基本思路</h3>
<p>在高中时我们都学习过二维线性规划，线性规划的结果总是能够在可行区域的顶点上取得。这一点在更一般的线性规划问题中也是成立的。线性规划的单纯形法的迭代步骤就是在这些“顶点”之间左右横跳，最终找到结果。</p>
<p>由于线性规划问题的约束具有 <span class="math inline">\(Ax=b\)</span>
的形式，并且 <span class="math inline">\(A\)</span>
是行满秩的，所以一个很自然的想法就是从中挑出 <span
class="math inline">\(x\)</span> 的 <span
class="math inline">\(m\)</span> 个分量（称为基变量），并记为 <span
class="math inline">\(x_B\)</span>，然后将剩下的部分部分记为 <span
class="math inline">\(x_N\)</span>。我们当然可以调整 <span
class="math inline">\(x\)</span> 中各个分量的顺序（同时也调整 <span
class="math inline">\(A\)</span> 与 <span
class="math inline">\(c\)</span>），使得基变量排在前面： <span
class="math display">\[\begin{align*}
x=[x_B,\;x_N]^{\mathrm T},\quad A=[A_B,\;A_N],\quad c =
[c_B,\;c_N]^{\mathrm T}.
\end{align*}\]</span> 由于 <span class="math inline">\(A\)</span>
是行满秩的，所以 <span class="math display">\[\begin{align}\label{xB xN}
Ax=b\quad\Leftrightarrow\quad A_Bx_B+A_Nx_N=b\quad\Leftrightarrow\quad
x_B=A_B^{-1}b-A_B^{-1}A_Nx_N.
\end{align}\]</span> 将它代入优化目标： <span
class="math display">\[\begin{align*}
c^{\mathrm T}x=c_B^{\mathrm T}x_B+c_N^{\mathrm T}x_N=c_B^{\mathrm
T}A_B^{-1}b+(c_N^{\mathrm T}-c_B^{\mathrm T}A_B^{-1}A_N)x_N.
\end{align*}\]</span></p>
<p>如果 <span class="math inline">\((c_N^{\mathrm T}-c_B^{\mathrm
T}A_B^{-1}A_N)\)</span> 的分量全是正数，由于 <span
class="math inline">\(x_N\geqslant0\)</span>，那么 <span
class="math inline">\(c^{\mathrm T}x\)</span> 的极小值就在 <span
class="math inline">\(x_N=0\)</span>
处取得。实际上可以证明，满足约束条件的区域的“顶点”一定是那些有 <span
class="math inline">\((n-m)\)</span> 个分量为 <span
class="math inline">\(0\)</span> 的点，即 <span
class="math inline">\(x_N=0\)</span> 的点。</p>
<p>如果 <span class="math inline">\((c_N^{\mathrm T}-c_B^{\mathrm
T}A_B^{-1}A_N)\)</span> 的第 <span class="math inline">\(i\)</span>
个分量是负数，就意味着我们可以增加 <span
class="math inline">\((x_N)_i\)</span> 来使 <span
class="math inline">\(c^{\mathrm T}x\)</span> 减小。很多情况下 <span
class="math inline">\((x_N)_i\)</span> 并不是无限制的，因为 <span
class="math inline">\(x_N\)</span> 会通过 (<span
class="math inline">\(\ref{xB xN}\)</span>) 影响 <span
class="math inline">\(x_B\)</span>。如果当 <span
class="math inline">\((x_N)_i\)</span> 增大到使某个 <span
class="math inline">\((x_B)_j\)</span> 变为 <span
class="math inline">\(0\)</span> 的程度，那么就应该调整 <span
class="math inline">\(B\)</span> 与 <span
class="math inline">\(N\)</span>，将 <span
class="math inline">\((x_B)_j\)</span> 加入 <span
class="math inline">\(x_N\)</span>，将 <span
class="math inline">\((x_N)_i\)</span> 加入 <span
class="math inline">\(x_B\)</span>。如果这种增加是无限制的，就说明目前的线性规划问题无解。</p>
<p>以上就是单纯形法的基本思路，当然还有很多细节没有补充，例如怎样选取
<span class="math inline">\((x_N)_i\)</span>
才不会导致死循环，如何选择初值等等。</p>
<h2 id="信赖域的-dogleg-方法">信赖域的 dogleg 方法</h2>
<p>在信赖域方法中，从 <span class="math inline">\(x_k\)</span> 确定出
<span class="math inline">\(x_{k+1}\)</span> 的方法有很多种，scipy
库中使用了一种名为 dogleg 的方法，它的基本做法是：</p>
<blockquote class="colorquote algorithm" ><ul>
<li><p>在信赖域方法中，用二次函数 <span
class="math inline">\(m_k(x)\)</span> 作为原函数的近似。</p></li>
<li><p>假设这是一个无约束优化问题，分别计算由梯度下降法得到的结果
(柯西点) <span class="math inline">\(x_{k+1}^C\)</span>
和由牛顿法得到的结果 <span
class="math inline">\(x^N_{k+1}\)</span>。</p></li>
<li><p>如果 <span class="math inline">\(x^N_{k+1}\)</span>
位于信赖域内，那么 <span
class="math inline">\(x_{k+1}=x^N_{k+1}\)</span>。如果 <span
class="math inline">\(x^N_{k+1}\)</span> 不在信赖域内，那么 <span
class="math inline">\(x_{k+1}\)</span> 取为折线 <span
class="math inline">\(x_{k}\)</span>-<span
class="math inline">\(x_{k+1}^C\)</span>-<span
class="math inline">\(x_{k+1}^N\)</span>（形象地称为
dogleg）与信赖域的交点。</p></li>
</ul>
</blockquote>
<p>这一方法可通过如下方式理解：接近 <span
class="math inline">\(x_k\)</span> 时，<span
class="math inline">\(m_k(x)\)</span>
的二阶项可忽略，下降最快的方向是负梯度方向，离 <span
class="math inline">\(x_k\)</span> 较远时，<span
class="math inline">\(m_k(x)\)</span>
的二阶项开始占主导，下降最快的方向趋于牛顿方向，dogleg
方法就是将这两种极端情形简单地拼接起来。</p>
<h2 id="约束优化问题的内点方法">约束优化问题的内点方法</h2>
<h3 id="约束优化问题的-kkt-条件">约束优化问题的 KKT 条件</h3>
<p>我们考虑的约束优化问题是： <span
class="math display">\[\begin{align}\label{original form constraint}
\text{最小化}\;f(x),\qquad x\;\text{满足}
\begin{cases}
E_i(x)=0:\quad i\in\mathcal E\\
I_j(x)\geqslant0:\quad j\in\mathcal I
\end{cases}.
\end{align}\]</span> 我们可以将它转换为另一种等价的形式： <span
class="math display">\[\begin{align}\label{equivalent-form-constraint}
\text{最小化}\;f(x,s)=f(x),\qquad x,s\;\text{满足}
\begin{cases}
E_i(x)=0:&amp;\quad i\in\mathcal E\\
I_j(x)-s_j=0:&amp;\quad j\in\mathcal I\\
s_j\geqslant0:&amp;\quad j\in\mathcal I
\end{cases}.
\end{align}\]</span></p>
<p>我们来考虑 (<span
class="math inline">\(\ref{equivalent-form-constraint}\)</span>) 的 KKT
条件。它的拉格朗日函数是： <span class="math display">\[\begin{align}
\mathcal L(x,s)= f(x)-\sum_{i\in\mathcal
E}\lambda_iE_i(x)-\sum_{j\in\mathcal
I}z_j[I_j(x)-s_j]-\sum_{j\in\mathcal I}z_js_j.
\end{align}\]</span> 从 <span class="math inline">\(\partial\mathcal
L(x,s)/\partial s=0\)</span> 的条件可以发现 <span
class="math inline">\(s_j\)</span> 与 <span
class="math inline">\([I_j(x)-s_j]\)</span>
项的拉格朗日乘子相等，这一点也可从原问题 (<span
class="math inline">\(\ref{original form constraint}\)</span>) 的 KKT
条件看出来。其他的 KKT 条件为： <span
class="math display">\[\begin{align}\label{KKT condition of constraint}
\begin{cases}
&amp;\nabla f(x)-\displaystyle\sum_{i\in\mathcal E}\lambda_i\nabla
E_i(x)-\sum_{j\in\mathcal I}z_j\nabla I_j(x)=0,\\
&amp;z_js_j=0,\quad z_j\geqslant0,\quad s_j\geqslant0:\quad j\in\mathcal
I,\\[3.5mm]
&amp;I_j(x)=s_j:\quad j\in\mathcal I,\\[3.5mm]
&amp;E_i(x)=0:\quad i\in\mathcal E.
\end{cases}
\end{align}\]</span> 内点法求解的是满足如下的近似 KKT 条件： <span
class="math display">\[\begin{align}
\begin{cases}
&amp;\nabla f(x)-\displaystyle\sum_{i\in\mathcal E}\lambda_i\nabla
E_i(x)-\sum_{j\in\mathcal I}z_j\nabla I_j(x)=0,\\
&amp;z_js_j=\mu&gt;0,\quad z_j\geqslant0,\quad s_j\geqslant0:\quad
j\in\mathcal I,\\[3.5mm]
&amp;I_j(x)=s_j:\quad j\in\mathcal I,\\[3.5mm]
&amp;E_i(x)=0:\quad i\in\mathcal E.
\end{cases}
\end{align}\]</span></p>
<p>如果 <span class="math inline">\(\mu=0\)</span>，以上近似 KKT
条件就是约束优化问题的 KKT 条件。如果 <span
class="math inline">\(\mu&gt;0\)</span>，那么求解以上近似 KKT 条件得到的
<span class="math inline">\(z\)</span> 与 <span
class="math inline">\(s\)</span> 都是大于 <span
class="math inline">\(0\)</span> 的，而 <span
class="math inline">\(s&gt;0\)</span> 意味着所有 <span
class="math inline">\(I(x)\)</span> 都大于 <span
class="math inline">\(0\)</span>，所以以上近似 KKT
条件的解都是可行区域的内点。可以想象，只要选取一个收敛于 <span
class="math inline">\(0\)</span> 的序列 <span
class="math inline">\(\{\mu_k\}\)</span>，对每个 <span
class="math inline">\(\mu_k\)</span> 求解以上近似 KKT
条件，那么得到的内点就会趋近于约束优化问题的解，这就是内点法的思路。</p>
<h3 id="对偶罚函数的-kkt-条件">对偶罚函数的 KKT 条件</h3>
<p>另一个与之相关的问题是对偶罚函数问题： <span
class="math display">\[\begin{align}\label{log panalty function}
\text{最小化}\;f(x,s)=f(x)-\mu\sum_{j\in\mathcal I}\log s_j,\qquad
x\;\text{满足}
\begin{cases}
E_i(x)=0:\quad i\in\mathcal E\\
I_j(x)=s_j:\quad j\in\mathcal I
\end{cases}.
\end{align}\]</span> 其中 <span
class="math inline">\(\mu&gt;0\)</span>。</p>
<p>这个问题的拉格朗日函数为： <span class="math display">\[\begin{align}
\mathcal L(x,s)= f(x)-\dfrac1t\sum_{j\in\mathcal I}\log
s_j-\sum_{i\in\mathcal E}\lambda_iE_i(x)-\sum_{j\in\mathcal
I}z_j[I_j(x)-s_j].
\end{align}\]</span> 它的 KKT 条件为： <span
class="math display">\[\begin{align}\label{KKT condition of log panalty}
\begin{cases}
&amp;\nabla f(x)-\displaystyle\sum_{i\in\mathcal E}\lambda_i\nabla
E_i(x)-\sum_{j\in\mathcal I}z_j\nabla I_j(x)=0,\\
&amp;-\mu\dfrac{1}{s_j}+z_j=0:\quad j\in\mathcal I,\\[3.5mm]
&amp;I_j(x)=s_j:\quad j\in\mathcal I,\\[3.5mm]
&amp;E_i(x)=0:\quad i\in\mathcal E.
\end{cases}
\end{align}\]</span> 可以看到，这个问题的 KKT 条件与内点法求解的近似 KKT
条件是等价的，只在第二个方程上存在差距。</p>
<link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>
    </div>

    
    
    

    <footer class="post-footer">

        

    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2024</span>
    <span class="with-love">
      <i class="fa fa-ghost"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">XMQ</span>
  </div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/pisces/" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动
  </div><script color="0,0,255" opacity="0.5" zIndex="-1" count="99" src="https://cdn.jsdelivr.net/npm/canvas-nest.js@1/dist/canvas-nest.js"></script>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/sidebar.js"></script><script src="/js/next-boot.js"></script>

  






  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"ams","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
